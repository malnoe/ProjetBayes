---
title: "Projet statistiques bayésiennes"
author: "Garance MALNOË et Matthias MAZET"
date: "`r Sys.Date()`"
output:
  bookdown::pdf_document2:
    toc: true
    toc_depth: 3
    number_sections: true
header-includes:
 \usepackage{float}
 \floatplacement{figure}{H}
 \usepackage{mdframed}
 \usepackage{caption}
documentclass: article
classoption: a4paper
geometry: margin = 1.5 cm
---


\newpage


# Introduction
Explication du contexte (lien avec la fiabilité ?) et explication des objectifs du projet.

# Simulations
```{r}
simulations <- function(horizon=100,dt_sim=0.01,dt_insp=5,alpha,beta,theta){
  # Simulations du processus continu
  grille_sim <- seq(0, horizon, by=dt_sim)
  a <- function(t) alpha*t^beta # val de a(t)
  shape <- diff(a(grille_sim)) # a(t) - a(s)
  increments <- rgamma(length(shape), shape=shape, scale=theta)
  X_sim <- c(0, cumsum(increments))
  
  # Calul des inspections
  grille_insp <- seq(0, horizon, by=dt_insp)
  n <- dt_insp / dt_sim
  X_insp <- X_sim[seq(1, length(X_sim), by = n)]
  
  list(grille_sim=grille_sim, X_sim=X_sim, grille_insp=grille_insp, X_insp=X_insp,t_insp=grille_insp)
}

plot_simulations <- function(simulations,main=""){
  plot(simulations$grille_sim,simulations$X_sim,col="blue",
       pch=20,
       xlab="t",ylab="Xt",main=main,
       cex.main=0.7,cex.axis=0.6)
  points(simulations$grille_insp,simulations$X_insp,col="red", pch=20)
  legend(x="bottomright",legend=c("Xt simulés","Xt inspectés"),fill=c("blue","red"))
}
```

## Variations de alpha
```{r}
set.seed(1)
# Simulations 1
dt_sim <- 0.01
dt_insp <- 5
beta <- 2
theta <- 2

list_alpha <- c(1,10,20)

simulations1 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=list_alpha[1],beta=beta,theta=theta)
plot_simulations(simulations1,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",list_alpha[1]," beta=",beta," theta=",theta,"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))

simulations2 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=list_alpha[2],beta=beta,theta=theta)
plot_simulations(simulations2,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",list_alpha[2]," beta=",beta," theta=",theta,"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))

simulations3 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=list_alpha[3],beta=beta,theta=theta)
plot_simulations(simulations3,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",list_alpha[3]," beta=",beta," theta=",theta,"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))
```
Commentaire : Pas d'impact sur la forme de la courbe seulement sur les valeurs prises par Xt. Proportionnel à alpha :
1 -> 20.000
10 -> 200.000
20 -> 400.000
Faire le lien avec les maths / la théorie.
Dégradations + rapide avec alpha + grand.

## Variations de beta
```{r}
set.seed(1)
# Simulations 1
dt_sim <- 0.01
dt_insp <- 5
alpha <- 1
theta <- 2

list_beta <- c(0.5,1,2)

simulations1 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=list_beta[1],theta=theta)
plot_simulations(simulations1,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",alpha," beta=",list_beta[1]," theta=",theta,"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))

simulations2 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=list_beta[2],theta=theta)
plot_simulations(simulations2,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",alpha," beta=",list_beta[2]," theta=",theta,"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))

simulations3 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=list_beta[3],theta=theta)
plot_simulations(simulations3,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",alpha," beta=",list_beta[3]," theta=",theta,"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))
```
Commentaire :
beta < 1, dégradation qui ralenti, forme concave
beta = 1, dégradation linéaire, forme linéaire
beta > 1, dégradation accélérée, forme convexe

## Variations de theta
```{r}
set.seed(1)
# Simulations 1
dt_sim <- 0.01
dt_insp <- 5
alpha <- 1
beta <- 1

list_theta <- c(0.5,1,2)

simulations1 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=beta,theta=list_theta[1])
plot_simulations(simulations1,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",alpha," beta=",beta," theta=",list_theta[1],"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))

simulations2 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=beta,theta=list_theta[2])
plot_simulations(simulations2,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",alpha," beta=",beta," theta=",list_theta[2],"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))

simulations3 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=beta,theta=list_theta[3])
plot_simulations(simulations3,main=paste0("Simulations d'un processur de gamma non-homogène (alpha=",alpha," beta=",beta," theta=",list_theta[3],"),\n avec un pas de ",dt_sim," et un pas d'inspection de ",dt_insp,"."))
```
Commentaire : Pas d'impact sur la forme de la courbe seulement sur les valeurs prises par Xt. Proportionnel à theta :
0.5 -> 50
1 -> 100
2 -> 200
Dégradation + rapide avec theta + grand
Faire le lien avec les maths / la théorie.

# Etude de l'inférence bayésienne

## Théorie

## Fonctions

```{r}
library(invgamma)
```

### Fonctions utiles, calcul du log des loi a posteriori

```{r}
# Fonction de calcul des ki
func_list_ki <- function(alpha, beta, t_insp){
  n <- length(t_insp)-1
  alpha * (t_insp[2:(n+1)]^beta-t_insp[1:n]^beta)
}


# Fonctions a posteriori pour alpha
## Lognormale(mu,sigma)
logpost_alpha_lognormale <- function(alpha, y, t_insp, beta, theta, mu_alpha, sigma_alpha){
  
  mu <- log(mu_alpha) - 0.5 * log(1 + sigma_alpha/(mu_alpha^2))
  sigma <- sqrt(log(1 + sigma_alpha/(mu_alpha^2)))

  list_ki <- func_list_ki(alpha, beta, t_insp)
  
  -log(alpha) - ((log(alpha)-mu)^2)/(2*sigma^2) +
    sum((list_ki - 1)*log(y) - list_ki*log(theta) - lgamma(list_ki))
}


## Gamma(k,s)
logpost_alpha_gamma <- function(alpha, y, t_insp, beta, theta, mu_alpha, sigma_alpha){
  k <- mu_alpha^2/sigma_alpha
  s <- sigma_alpha/mu_alpha  

  list_ki <- func_list_ki(alpha, beta, t_insp)

  (k-1)*log(alpha) - alpha/s +
    sum((list_ki - 1)*log(y) - list_ki*log(theta) - lgamma(list_ki))
}

# Fonctions a posteriori pour beta
## Lognormale(mu,sigma)
logpost_beta_lognormale <- function(beta, y, t_insp, alpha, theta, mu_beta, sigma_beta){
  mu <- log(mu_beta)- 0.5 * log(1 + sigma_beta/(mu_beta^2))
  sigma <- sqrt(log(1 + sigma_beta/(mu_beta^2)))

  list_ki <- func_list_ki(alpha, beta, t_insp)

  -log(beta) - ((log(beta)-mu)^2)/(2*sigma^2) +
    sum((list_ki - 1)*log(y) - list_ki*log(theta) - lgamma(list_ki))
}

## Uniforme(a,b)
logpost_beta_unif <- function(beta, y, t_insp, alpha, theta, mu_beta, sigma_beta){
  a <- mu_beta - sqrt(3*sigma_beta)
  b <- mu_beta + sqrt(3*sigma_beta)

  if(beta < a || beta > b) return(-Inf)
  list_ki <- func_list_ki(alpha, beta, t_insp)
  sum((list_ki - 1)*log(y) - list_ki*log(theta) - lgamma(list_ki))
}
```

### Fonction algorithme de Gibbs-Metropolis-Hastings
```{r}
algo_Gibbs_Metropolis_Hastings <- function(logpost_alpha,mu_alpha,sigma_alpha,tau_alpha,alpha0,
                                           logpost_beta,mu_beta,sigma_beta,tau_beta,beta0,
                                           mu_theta,sigma_theta,theta0,
                                           data,t_insp,K,burnin){
  
  # logpost_alpha <- fonction pour le calcul de log(posteriori(alpha~))
  # mu_alpha <- moyenne de l'expert
  # sigma_alpha <- confiance en l'expert
  # tau_alpha <- variation de la normale
  # alpha0 <- initialisation de alpha pour chaine de markov
  
  # logpost_beta <- fonction pour le calcul de log(posteriori(beta~))
  # mu_beta <- moyenne de l'expert
  # sigma_beta <- confiance en l'expert
  # tau_beta <- variation de la normale
  # beta0 <- initialisation de beta pour chaine de markov
  
  # mu_theta <- moyenne de l'expert
  # sigma_theta <- confiance expert
  # theta0 <- initialisation de theta pour la chaine de markov
  
  # data <- x1,...,xn observations (à transformer en y1,...,yn) les incréments
  # t_insp <- t1,...,tn les temps d'inspection
  # K <- nombre d'itérations des algorithmes
  # burnin <- nombre d'itérations à supprimer au début
  
  # Transformation des données en incréments y_i = x(t_i) - x(t_i-1)
  y <- data[2:length(data)] - data[1:length(data)-1]
  n <- length(y)
  
  # Calcul du a et b de l'inverse gamma pour simuler theta à partir de esp_theta et de var_theta
  a_theta <- mu_theta^2/sigma_theta+2
  b_theta <- (a_theta-1)*mu_theta
  
  # Initialisation des vecteurs de résultats
  res_alpha <- numeric(K+1)
  res_alpha[1] <- alpha0
  res_beta <- numeric(K+1)
  res_beta[1] <- beta0
  res_theta <- numeric(K+1)
  res_theta[1] <- theta0
  
  # Boucle sur k in 2,...,K+1 avec calcul de alpha^(k),beta^(k) et theta^(k)
  for(k in 2:(K+1)){
    # 1. Simulation de theta^(k)
      # Calcul des k_i = alpha(ti^beta - ti-1^beta) avec alpha=alpha^(k-1) et beta=beta^(k-1)
    list_ki <- res_alpha[k-1]*(t_insp[2:(n+1)]^res_beta[k-1]-t_insp[1:n]^res_beta[k-1])
      # Calcul de a_bis = a + somme ki
    a_bis <- a_theta+sum(list_ki)
      # Calcul de b_bis = b + somme yi
    b_bis <- b_theta+sum(y)
    res_theta[k]  <- rinvgamma(1,shape=a_bis,rate=b_bis)
    
    # 2. Simulation de alpha^(k) (MH)
    alpha_tilde <- rnorm(1, mean = res_alpha[k-1], sd = tau_alpha)
    if(alpha_tilde <= 0){
      res_alpha[k] <- res_alpha[k-1]
    } 
    else {
      first_log  <- logpost_alpha(alpha_tilde, y, t_insp, res_beta[k-1], res_theta[k], mu_alpha, sigma_alpha)
      second_log <- logpost_alpha(res_alpha[k-1], y, t_insp, res_beta[k-1], res_theta[k], mu_alpha, sigma_alpha)
      log_r <- first_log - second_log
      u <- runif(1)
      res_alpha[k] <- ifelse(log(u) <= log_r, alpha_tilde, res_alpha[k-1])
    }
    
    # 3. Simulation de beta^(k) (MH)
    beta_tilde <- rnorm(1, mean = res_beta[k-1], sd = tau_beta)
    if(beta_tilde <= 0){
      res_beta[k] <- res_beta[k-1]
    } 
    else {
      first_log  <- logpost_beta(beta_tilde, y, t_insp, res_alpha[k], res_theta[k], mu_beta, sigma_beta)
      second_log <- logpost_beta(res_beta[k-1], y, t_insp, res_alpha[k], res_theta[k], mu_beta, sigma_beta)
      log_r <- first_log - second_log
      u <- runif(1)
      res_beta[k] <- ifelse(log(u) <= log_r, beta_tilde, res_beta[k-1])
    }

  }
  
  # Résultat avec burnin
  return(list(res_theta=tail(res_theta,n=K-burnin),
              res_alpha=tail(res_alpha,n=K-burnin),
              res_beta=tail(res_beta,n=K-burnin)))
  
}
```

### Fonction calcul des estimateurs de vraisemblance par optimisation

```{r}
# - vraisemblance à minimiser pour trouver les MLE.
negative_vraisemblance <- function(vect_param, y, t_insp){
  alpha <- vect_param[1]
  beta  <- vect_param[2]
  theta <- vect_param[3]

  list_ki <- func_list_ki(alpha, beta, t_insp)

  if(any(!is.finite(list_ki)) || any(list_ki <= 0) || !is.finite(theta) || theta <= 0) return(1e100)
  if(any(!is.finite(y)) || any(y <= 0)) return(1e100)

  -(sum((list_ki - 1) * log(y) - (y/theta) - list_ki * log(theta) - lgamma(list_ki)))

}

# Fonction calculant les MLE via optimisation.
mle <- function(alpha0, beta0, theta0, data, t_insp){
  y <- data[2:length(data)] - data[1:(length(data)-1)]
  init <- c(alpha0, beta0, theta0)
  fit <- optim(
    par = init,
    fn  = negative_vraisemblance,
    y = y,
    t_insp = t_insp,
    method = "BFGS",
    control = list(maxit = 5000, reltol = 1e-10)
  )
  list(
    alpha = fit$par[1],
    beta  = fit$par[2],
    theta = fit$par[3],
    converged = (fit$convergence == 0)
  )
}
```

## Résultats

```{r}
set.seed(1)
# Simulations 1
dt_sim <- 0.01
dt_insp <- 5
alpha <- 1
beta <- 2
theta <- 2
simulations1 <- simulations(dt_sim=dt_sim,dt_insp=dt_insp,alpha=alpha,beta=beta,theta=theta)
```


```{r}


logpost_alpha <- logpost_alpha_gamma
mu_alpha <- alpha+0.01
sigma_alpha <- 1
tau_alpha <- 0.5
alpha0 <- mu_alpha

logpost_beta <- logpost_beta_lognormale
mu_beta <- beta+0.01
sigma_beta <- 1
tau_beta <- 0.5
beta0 <- mu_beta

mu_theta <- theta+0.01
sigma_theta <- 1
theta0 <- mu_theta

data <- simulations1$X_insp
t_insp <- simulations1$t_insp
K <- 1000
burnin <- 100

res_GMH <- algo_Gibbs_Metropolis_Hastings(logpost_alpha,mu_alpha,sigma_alpha,tau_alpha,alpha0,
                                           logpost_beta,mu_beta,sigma_beta,tau_beta,beta0,
                                           mu_theta,sigma_theta,theta0,
                                           data,t_insp,K,burnin)

res_MLE <- mle(alpha0,beta0,theta0,data,t_insp)
```



Visualisations :
Fixer un alpha, beta et theta.
Pour chaque coombinaison de lois a priori Faire grille avec chaque cas de figure (expert bon ou mauvais, confiance faible ou élevée). (ggarance 2 cols 2 lignes)
Visualisation avec titre commun les vrais paramètres et lois a priori choisies, titre perso avec cas de figure expert et simulations avec vrais paramètres et simulations avec paramètres estimés bayes et simulations avec MLE.

-> Graphe de convergence pour chaque estimateur.

Commentaire :
Il est important de vérifier la convergence de GMH et de MLE (GMH en faisant le plot de res_param et voir si bcp oscillations ou pas et pour MLE avec converged )



# Conclusion
